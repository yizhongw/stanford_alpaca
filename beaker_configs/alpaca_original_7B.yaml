version: v2
description: open-instruct-alpaca-7B-original
tasks:
  - name: open-instruct-alpaca-7B-original
    image:
      beaker: Yizhongw03/stanford-alpaca
    command: [
      "torchrun", "--nproc_per_node=4", "--master_port=10086", "train.py",
      "--model_name_or_path", "/hf_llama_models/llama-7b",
      "--tokenizer_name", "/hf_llama_models/tokenizer",
      "--data_path", "/data/stanford_alpaca/alpaca_data.json",
      "--bf16", True,
      "--output_dir", "/output/",
      "--num_train_epochs", 3,
      "--per_device_train_batch_size", 4,
      "--per_device_eval_batch_size", 4,
      "--gradient_accumulation_steps", 8,
      "--evaluation_strategy", "no",
      "--save_strategy", "steps",
      "--save_steps", 200,
      "--save_total_limit", 5,
      "--learning_rate", 2e-5,
      "--weight_decay", 0.,
      "--warmup_ratio", 0.03,
      "--lr_scheduler_type", "cosine",
      "--logging_steps", 1,
      "--fsdp", "full_shard auto_wrap",
      "--fsdp_transformer_layer_cls_to_wrap", "LlamaDecoderLayer",
      "--tf32", True,
      "--report_to", "wandb",
    ]
    envVars:
      - name: CUDA_DEVICE_ORDER
        value: PCI_BUS_ID
      - name: TRANSFORMERS_CACHE
        value: ./cache/
      - name: WANDB_PROJECT
        value: open-instruct
      - name: WANDB_WATCH
        value: false
      - name: WANDB_LOG_MODEL
        value: false
    datasets:
      - mountPath: /data/stanford_alpaca
        source:
          beaker: Yizhongw03/stanford_alpaca
      - mountPath: /hf_llama_models
        source:
          beaker: Yizhongw03/hf_llama_model_7B
    result:
      # Beaker will capture anything that's written to this location and store it in the results
      # dataset.
      path: /output
    resources:
      gpuCount: 4
    context:
      cluster: ai2/yizhongw-4xa100-80gb
      priority: high